# Prior-knowledge
![[microcontroller.pdf]]
![[RTS.CS-prior-knowledge.pdf]]
# Introduction to operating systems
## Slides
![[OS-01-Introduction.pdf]]
## Info
### Slide Pics and Book Pages
1.1 to 1.5 + 1.10.6. and 2.1.
![[DMA.png|300]]![[interrupt driven IO cycle.png|300]]
![[storage device hierarchy.png|300]]![[definitions of computer system components.png|300]]

![[interrupr driven operations.png|300]]![[application and OS communication.png|300]]

![[dual mode operation.png|300]]![[example of read operation.png|300]]
![[traps vs interrupts.png|300]]![[user vs kernel mode.png|300]]

![[intro information.png]]
### Trap/Interrupt and System Call
A **[trap](http://en.wikipedia.org/wiki/Kernel_trap)** is an exception in a user process. It's caused by division by zero or invalid memory access. It's also the usual way to invoke a kernel routine (a [system call](http://en.wikipedia.org/wiki/System_call)) because those run with a higher priority than user code. Handling is synchronous (so the user code is suspended and continues afterwards). In a sense they are "active" - most of the time, the code expects the trap to happen and relies on this fact.

An **[interrupt](http://en.wikipedia.org/wiki/Interrupt)** is something generated by the hardware (devices like the hard disk, graphics card, I/O ports, etc). These are asynchronous (i.e. they don't happen at predictable places in the user code) or "passive" since the [interrupt handler](https://en.wikipedia.org/wiki/Interrupt_handler) has to wait for them to happen eventually. Called to deal with the cause of interrupt.

![[some info about trapsinterrupts.png|400]]
### Steps in Processing a System Call
1. Push parameters.
2. Call library function.
3. Set up system call code in a register.
4. Trap to kernel.
5. Handler executes action.
6. Return to user mode.
### Further Notes (DMA, Memory, Other Concepts)

- How CPU interface with device to coordinate transfer to avoid increasing load on CPU processing?: **CPU can initiate DMA by writing special registers that can be accessed by device directly.**
- When device completes operation?: **It generates an interrupt (hardware generated), to indicate completion of operation**
- Device(DMA) and CPU can access memory simultaneously but compete for memory bus to access memory.
- A DMA may impact negatively the execution time of a program

>[!NOTE] Three general methods for passing parameters to the operating system:
>1. **Pass parameters in registers**: Parameters are directly passed using CPU registers, which is fast and efficient for small data.
>2. **Registers pass starting addresses of blocks of parameters**: Instead of directly passing all parameters, a register can store the starting address of a memory block that contains the parameters.
>3. **Parameters pushed onto the stack**: The program places (or pushes) parameters onto the stack, and the operating system retrieves them by popping the stack.

[Mechanism and policy](https://en.wikipedia.org/wiki/Separation_of_mechanism_and_policy) should be separate to ensure systems are easier to modify. No two system installations are identical, so each installation might need to tune the operating system to meet specific needs. By separating mechanism and policy:
- Policies can be changed as needed without altering the underlying mechanism.
- Mechanisms remain constant, simplifying system flexibility and maintenance.

**Policy** define what you want system to do, and **Mechanism** how to do it

>[!NOTE] [Tight Coupling](https://en.wikipedia.org/wiki/Coupling_(computer_programming)) in System Components
>- **Example**: Virtual memory subsystem & storage subsystem.
>- **Reasons for Tight Coupling**:
>	1. Files mapped into virtual memory space.
>	2. Virtual memory uses storage as a backing store for non-resident pages.
>	3. File system updates buffered in memory before flushing to disk.
>- **Challenge**: Requires careful coordination for memory and storage usage, making layered design difficult.

![[OS vs mikrokernel OS.png|400]]
[Mikrokernel](https://en.wikipedia.org/wiki/Microkernel) more secure due to as more operations are done in the application layer or user mode, but relies on more IPC, which could be good but hard to control at scale.

### Why use OS
1. Improves portability.
2. Manages concurrency
3. Provides a simplified view of the execution platform
4. Provides support for shared functionality
# Processes, threads and scheduling
# Processes
## Slides
![[OS-02-process.pdf]]
![[OS-02-Process-Preparation-2021.pdf]]
![[OS-04-Scheduling-intro.pdf]]

## Info
### Slide & Book Pics
3.1-3.3 
![[Processes lecture.png]]
### Active Process Scheduling/Creation/Termination

![[process scheduling.png|300]]![[process scheduling explanatory.png|300]]
![[process states.png|300]]![[context switch.png|300]]

![[process creation.png|300]]![[process creation-1.png|300]]
>[!NOTE] Process Creation - Cont
>Parent and children execute **concurrently**, parent awaits child's termination.
>**Address space:** Linux (child a copy of parent), Windows (new program loaded into its address space).
>
>Process creation example: Slides Process: 24

![[posix API.png|300]]![[forking in posix.png|300]]
Portable Operating System Interface (POSIX) API for process creation / termination
![[process termination.png|400]]
>[!NOTE] Parent Exiting
>If the parent is exiting 
>	- Some operating systems do not allow the child to continue if its parent terminates, in which case all children are terminated 
>	- cascading termination − Some operating systems attach orphans to a ‘grandfather’ process (e.g. init)

![[Process termination async.png|300]]
### Interprocess Communication (IPC)
![[type of memory access for IPC.png|300]]![[messafe passing between processes.png|300]]
![[shared memory access.png|300]]
### Preliminary info on Scheduling
- A process may have **multiple** tasks, if we have one thread, then only one task can use the resource at a time.
- One task does not use a resource constantly so scheduling must occur so threads can be reassigned to different tasks during **process execution.**

![[resource scheduling allocation.png|300]]![[2INC0/attachments/scheduling example.png|300]]

>[!NOTE] Scheduling Policy
> A scheduling policy represents strategy for allocating a resource to a task.
> **Decision based:** Scheduling criteria: 
> - (task attribute: [deadline, response time])
> - (current state: [ready processes, available/required resources])
> - pre-computed lookup table

![[metrics for quality of cpy scheuduling.png|300]]![[time attributes of task.png|300]]
![[cpu scheduling.png|300]]![[decision mode.png|300]]

>[!NOTE] Important Info from CPU Scheduling
>- **response time:** time it takes when request submitted until response, not entire output.
>- **Preemptive** scheduling allows a running process to be interrupted by a high priority process, whereas in **non-preemptive** scheduling, any new process has to wait until the running process finishes its CPU cycle.
>- [Preemptive and Non-Preemptive Scheduling - GeeksforGeeks](https://www.geeksforgeeks.org/preemptive-and-non-preemptive-scheduling/)
>
>![[preemptive scheduling.png|200]]![[non-preemtive scheduling.png|200]]

### Process & Threads
![[concurrency.png|300]]![[process.png|300]]
>[!NOTE] Heap vs Stack
>**Heap memory is used by all the parts of the application whereas stack memory is used only by one thread of execution**. Whenever an object is created, it's always stored in the Heap space and stack memory contains the reference to it.
>
>**Text section:** the executable code
>**Data section:** global variables (static data size)
>**Heap section:** memory that is dynamically allocated during program run time
>**Stack section:** temporary data storage when invoking functions (function parameters, return addresses, local variables)

>[!NOTE] Process & Concurrency
>**Parallelism:** if the processor has more than one core, two or more different process can make **simultaneous progress**.
>
>**Concurrency:** The same resource (e.g., a core or network driver) can be time shared between processes -> **improve responsiveness**.
>
>If a process is **waiting** (e.g., to get access to a resource, or to receive data from another process), then the **kernel** can **schedule another process** -> **more efficient use of the system resource**.


![[memory layout of a program, heap.png|300]]![[process disadvantage.png|300]]


![[2INC0/attachments/Process Control Block.png|300]]![[pcb.png|300]]

![[creation process.png|400]]
>[!NOTE] **Example: Create New Processes**
>- **fork()**:
>  - Creates a new process (child) that is an identical copy of the parent process in a separate memory space.
>  - Only the return value differs:
>    - `0` for the child process.
>    - `child-pid` for the parent process.
>    - Negative value (`< 0`) if an error occurs.
>
>- **exec()**:
>  - Overwrites the calling process (child) with the specified executable program (e.g., `/bin/ls`).
>  - If successful, no code after `exec()` is executed.
>  - If `exec()` fails, an error is printed using `perror()` and the process exits.
>
>- **Parent and Child Process Behavior**:
>  - **Child process (`if (child == 0)`):**
>    - Executes the `ls` command using `execlp("/bin/ls", "ls", arg0, arg1, ..., NULL)`.
>    - If `execlp()` fails, an error message is printed, and the process exits.
>  - **Parent process (`else`):**
>    - Waits for the child process to complete using `wait(&status)`.
>    - Handles its own logic (e.g., returning from the routine).
>
>- **Error Handling**:
>  - If `fork()` fails:
>    - Prints an error message using `perror("fork")` and exits the parent process.
>  - If `exec()` fails:
>    - Prints an error message using `perror("execlp")` and exits the child process.


![[termination of children.png|300]]![[process terminaiton.png|300]]

![[process creation example.png|300]]![[process execution.png|300]]

### Process Creation Exercises (Fork Analysis)
**`exec()` system call:** When a child process calls `exec()`, it replaces its own memory space with a new program, effectively becoming a new process. However, this doesn't affect the parent's ability to wait for the child, but in certain cases (like daemonized processes), the parent may choose not to wait.

**`smith > 0` (parent processes)**.
**`smith == 0` (child processes)**.
#### Key Concepts
1. **`fork()` basics**:
   - Each call to `fork()` doubles the number of processes.
   - Parent continues where `fork()` was called; child starts executing the same code from that point.
2. **Parent vs Child**:
   - **Parent**: `fork()` returns **child PID** (`> 0`).
   - **Child**: `fork()` returns **0**.
#### Step-by-Step Guide
#### 1. Analyze Initial State
- Start with **1 process (parent)**.
#### 2. Identify `fork()` Calls
- Count **direct `fork()`** calls.
- Check conditions (`if`) to determine **which processes execute the `fork()`**.
#### 3. Loop Handling
- For each loop:
  - Count how many processes enter the loop.
  - Track new processes created in each iteration.
#### 4. Total Processes
- Sum **initial process + all new processes** from `fork()`.
#### Example Walkthrough
**Code**: 13 Processes at end
```c
smith = fork();               // 1 new process
for (int j = 0; j < 3; j++) {
    if (smith == 0) { smith = fork(); } 
}                             // 3 new processes (child only)
for (int k = 0; k < 2; k++) {
    if (smith > 0) { smith = fork(); }
}                             // 2 new processes (parent only)
```

When a process calls `exit()`, the status is returned to the parent if it was waiting. Then, the OS takes the process’ resources away. Alternatively, the parent may terminate execution of children (abort). If no parent is waiting, the child process is a **zombie**. If the parent terminated without invoking `wait()`, the child process is an **orphan**.
### Processes Summary
- **Definition**: A program in execution with its own memory context (text, stack, heap, data).
- **Key Components**:
  - **Text**: Code.
  - **Stack**: Local vars, function params, return addresses.
  - **Heap**: Dynamically allocated data.
  - **Data**: Global variables.
- **Process Control Block (PCB)**: Contains process state, program counter, registers, scheduling info, and more.
- **States**:
  - **New**: Being created.
  - **Ready**: Awaiting CPU.
  - **Running**: Executing.
  - **Waiting**: Awaiting an event.
  - **Terminated**: Finished.
- **Process Creation**:
  - Parent creates children via `fork()`.
  - Options: Full/partial/no resource sharing, concurrent/sequential execution.
  - `exec()` replaces process memory with a new program.
  - `wait()`/`waitpid()`: Parent waits for child termination.
- **Key System Calls**:
  - `fork()`: Duplicate process.
  - `exec()`: Replace process with a new program.
  - `exit()`: Terminate process.
  - `wait()`: Block until child exits.
- **Parallelism vs Concurrency**:
  - **Parallelism**: Multiple cores, simultaneous progress.
  - **Concurrency**: Time-sharing a single resource.
- **Overhead**: Context switching (save/reload state).

![[global variables shared between 2 processes.png|400]]
# Threads and Scheduling
## Slides
![[OS-03-thread_scheduling.pdf]]

## Info
### Slides & Book
4.1-4.3 + 4.6.5
![[threads and scheduling.png]]
![[more scheduling and IPC.png]]
### Threads
![[process vs thread.png|300]]![[threds share.png|300]]
>[!NOTE] Threads are Faster than Processes
>- **Thread creation and termination** is much faster (no memory address space copy needed).
>- **Context switching between threads** of the same process is much faster (no need to switch the whole address space, only swap cpu registers content).
>- **Communication between threads** is faster and more at the programmer’s hand than between processes (shared address space).

![[threds for reason.png|300]]![[thread posix.png|300]]

#### Managing User and Kernel Threads
![[basic view.png|300]]![[actual view.png|300]]
![[user and kernel threads.png|300]]![[multi threaded user level.png|300]]

Having tabs as separate processes instead of threads make it so if chrome crashes tabs are unaffected since only the visual interface of the browser crashes, if it were separate threads they would of crashed as they are in same process.

![[process thread creation.png|400]]
##### Multithreading
A **concurrent system** supports more than one task by allowing all the tasks to make progress. In contrast, a **parallel system** can perform more than one task simultaneously.

**Single core systems** can only be **concurrent** as inside 1 processor, all threads COULD be made to work together as they share resources. But only with **multi core or multi processor systems** **parallel** systems are possible to execute actual processes at same time.
![[many to one model.png|300]]![[one to one model.png|300]]
![[many to many model.png|300]]![[two level model.png|300]]

![[thread switching.png|300]]![[thread pools.png|300]]

>[!NOTE] **Scheduler Activations and LWPs in Multithreaded Programs**
>
> Multithreaded programs using the **many-to-many** or **two-level** models require communication between the kernel and the thread library to dynamically adjust the number of kernel threads for optimal performance. This coordination is facilitated by lightweight processes (LWPs), which act as virtual processors connecting user threads to kernel threads.
>
> ### Key Points:
> - **LWP Structure**:
>   - LWPs appear as virtual processors to the user-thread library.
>   - Each LWP is linked to a kernel thread, which the OS schedules on physical processors.
>   - Blocking behavior propagates up the chain:
>     - If a kernel thread blocks (e.g., waiting for I/O), the associated LWP and user thread also block.
>
> - **Efficiency Considerations**:
>   - A CPU-bound application on a single processor needs only one LWP.
>   - I/O-intensive applications may require multiple LWPs, especially for concurrent blocking system calls.
>   - For example, five simultaneous file-read requests require five LWPs; otherwise, requests exceeding the available LWPs must wait.
>
> - **Scheduler Activation**:
>   - The kernel provides virtual processors (LWPs) to applications, allowing user threads to schedule on available virtual processors.
>   - **Upcalls**:
>     - The kernel informs the application of events through upcalls, which are handled by an upcall handler running on a virtual processor.
>     - Example event: When a thread is about to block:
>       1. The kernel makes an upcall identifying the blocking thread.
>       2. A new virtual processor is allocated to handle the upcall.
>       3. The upcall handler saves the state of the blocking thread and schedules another eligible thread.
>     - When the blocking event resolves:
>       - Another upcall informs the thread library, marking the previously blocked thread as eligible to run.
>       - The application schedules an eligible thread on an available virtual processor.

>[!NOTE] Context vs Model Switch
>- mode switch: kernel <-> user mode. Takes very little time.
>- context switch: happens for process switch - stores the state of a process while restoring the state of another process (costs time).

![[multithreading user vs kernel threads.png|400]]
### Threads Summary
- **Definition**: Dispatchable unit within a process sharing the same address space.
- **Advantages**:
  - Faster creation, switching, and communication than processes.
  - Shared memory space reduces IPC overhead.
- **Models**:
  - **Many-to-One**: Single kernel thread for multiple user threads (no concurrency).
  - **One-to-One**: Each user thread maps to a kernel thread (higher overhead).
  - **Many-to-Many**: Multiple user threads map to a limited number of kernel threads.
  - **Two-Level**: Many-to-Many with optional user-thread-to-kernel-thread binding.
- **Thread Pool**: Pre-created threads waiting for tasks improve performance.
- **Execution**:
  - **Kernel-managed**: Kernel handles state.
  - **Library-managed**: Requires manual yielding.
- **Multithreading Advantages**:
  - Increases concurrency and responsiveness.
  - Exploits multi-core systems.

![[shreads share global variable.png|400]]
### Scheduling

>[!NOTE] Types of Scheduling
>**Processor scheduling:** decide which task (thread/process) executes on each core at every given time. **(Policy Implemented in Kernel Thread area)**
>**Memory scheduling:** decide which memory page of a process is loaded in each page frame of the main memory at every given time.

![[scheduling terminology.png|300]]![[time attributes of  task.png|300]]

1st one -> dual core, 2nd one -> single core
![[FCFS.png|300]]![[fcfc 2.png|300]]
![[sjf.png|300]]![[sjf 2.png|300]]
![[srtf.png|300]]![[srtf 2.png|300]]
![[round robin scheduling.png|300]]![[rr.png|300]]
![[rr 2.png|300]]![[ps.png|300]]
![[rms.png|300]]![[dms.png|300]]
![[edfs.png|300]]![[mqs.png|300]]
![[mqs-1.png|300]]![[mfqs.png|300]]
![[mfqs-1.png|300]]

[Round Robin Algorithm Tutorial (CPU Scheduling) - YouTube](https://www.youtube.com/watch?v=aWlQYllBZDs)

>[!NOTE] Priority Inversion
>The situation in scheduling that a high priority process is blocked by a process of middle priority, through blocking on a resource shared between this high and a (third) low priority process. During this blocking, arbitrary processes of middle priority can overtake. An example is found on the slides.



### Scheduling Summary
- **Terminology**:
  - **Decision Modes**: Preemptive (interrupt-driven) vs Non-preemptive (voluntary).
	  - [Preemptive and Non-Preemptive Scheduling - GeeksforGeeks](https://www.geeksforgeeks.org/preemptive-and-non-preemptive-scheduling/)
	  - non-preemptive scheduling is rigid as the current process continues to access the CPU even if other processes enter the queue.
  - **Priority Function**: Determines next task.
  - **Metrics**: Waiting time, response time, turnaround time, throughput.
- **Scheduling Algorithms**:
  - **FCFS**: First come, first served; non-preemptive.
  - **SJF**: Shortest job first; optimal waiting time but requires burst prediction.
  - **RR**: Round Robin; equal time slices, good for responsiveness.
  - **Priority Scheduling**: High-priority tasks first; may cause starvation (solved by aging).
  - **Multilevel Queue**: Partition ready queue with different policies per queue.
  - **Rate Monotonic (RM)**: Higher priority for shorter periods; real-time systems.
  - **Earliest Deadline First (EDF)**: Priority for tasks nearing deadlines.
- **Issues**:
  - **Priority Inversion**: Resolved by priority inheritance (adjust task priority dynamically).
### Other
A concurrent system supports more than one task by allowing all the tasks to make progress. In contrast, a parallel system can perform more than one task simultaneously.
![[task and data parallelism.png|300]]
- **With LWP**: A user thread performing a file read blocks its associated LWP, but other user threads can continue running on their LWPs.
- **Without LWP**: A user thread performing a file read blocks, and since there's no LWP to isolate the blocking, all user threads in the application are stalled.

Fundamentally, then, **data parallelism** involves the distribution of data across multiple cores, and **task parallelism** involves the distribution of tasks across multiple cores
#### Inter-Process Communication (IPC)
- **Message Passing**: Small data exchange, suitable for distributed systems.
  - **Blocking**: Waits for synchronization.
  - **Non-blocking**: Continues execution.
- **Shared Memory**: Kernel-allocated space accessible by processes.
  - Faster but prone to interference (requires synchronization).
#### Miscellaneous
- **Context Switch**: Switching processes involves saving/restoring states (time-consuming).
- **Mode Switch**: Switching between user and kernel modes (minimal overhead).
- **Busy Waiting**: Acceptable when short or the only option for synchronization.

[Race Condition and Deadlock | CloudxLab Blog](https://cloudxlab.com/blog/race-condition-and-deadlock/)
[Fixed-priority pre-emptive scheduling - Wikipedia](https://en.wikipedia.org/wiki/Fixed-priority_pre-emptive_scheduling#:~:text=Fixed%2Dpriority%20preemptive%20scheduling%20is,are%20currently%20ready%20to%20execute.)
for a non-preemptive the time is the decider or what completes first, not flexible on priority.
# Concurrency and Synchronization
# Atomicity and Interference, Traces and Mutual Exclusion
## Slides
![[OS-03-slides_preparation_videos.pdf]]

![[OS-03-atomicity_concurrency.pdf]]

## Info
### Slides & Book
6.1-6.6, 6.8.1
![[atomicity and interference.png]]
### Atomicity, traces & Concurrency

![[atomic variables.png|300]]![[concurrency traces.png|300]]
![[traces of concurrent tasks.png|300]]![[traces sequence of atomic actions.png|300]]

![[example of concurrent traces.png|300]]![[traces structure.png|300]]

>[!NOTE] Summary of Atomicity, Traces and Concurrency
>- **Shared variables:** accessible to several tasks (processes/threads).
>  .
>- **Private variables:** accessible only to a single task.
>  .
>- **Atomic action:** Indivisible step in the evolution of an executed program. 
>	- **typically, assignments and tests in a program.**
>	- **sufficient at program level: single reference to shared variables in statement.**
>		- ignoring possible optimizations and reordering by compiler.
>	- (outcome only depends on system state when the action starts and not on what runs concurrently during the execution of the action).
>	.  
>- **Concurrent execution:** interleaving of atomic actions.
>  .
>- **Interference:** disturbing others' assumptions about the state.
>	- **usually caused by "bad" interleaving**
>	- **particularly with shared variables**
>	.  
>- **Race conditions:** situation in which correctness depends on execution order of concurrent activities ("bad interference").
>  .
>- **Trace:** **Sequence of atomic actions** in the execution of a program, typically a sequence of **assignments** and **tests**.
>  .
>- **Concurrent trace:** Trace made of an **interleaving** of atomic actions of two or more tasks.
>  .
>- **Single reference rule:** Statement(expression) may be regarded as **atomic** if it makes **at most one reference to a shared variable.**


![[atomicity example.png|300]]![[threads handled in traces.png|300]]

![[stupid sync exercise.png|400]]
### Synchronization
![[purpose of sync.png|300]]![[boolean sync.png|300]]
![[adding guards.png|300]]![[assertions.png|300]]
![[swapping guard and assignment.png|300]]![[deadlock caused.png|300]]

![[shared variable mutual exclusion.png|300]]![[requirements on synchronization solutions.png|300]]

>[!NOTE] Critical Section Problem

### Peterson's Algorithm
![[peterson algorithm.png|400]]

**Check Correctness:** mutual exclusion and absence of deadlock (using traces or better **detailed annotation**).
![[no deadlock proof.png|300]]![[mutual exclusion.png|300]]
![[mutual exclusion p2.png|300]]![[limitations of peterson algo.png|300]]

### Synchronization with mutexes

![[mutual exclusion using mutexes.png|300]]![[sync using mutex.png|300]]
![[posix mutex.png|300]]

**Challenges with priority scheduling when protecting shared resource accesses**
![[blocking using mutex in priority scheduling.png|300]]![[blocking high priority with middle priority task.png|300]]

![[priority inversion.png|400]]

### Summary of Mutex
[Peterson’s Solution - YouTube](https://www.youtube.com/watch?v=gYCiTtgGR5Q&t)
[Test and Set Lock - YouTube](https://www.youtube.com/watch?v=5oZYS5dTrmk)
[Semaphores - YouTube](https://www.youtube.com/watch?v=XDIOC2EY5JE)
![[operating systems threads processing requirements.png|400]]
**Mutexes protect the process's critical resources, whereas spinlocks protect the interrupt request (IRQ) handler's critical sections**. Mutexes put contenders to sleep until the lock is acquired, whereas spinlocks infinitely spin in a loop (consuming CPU) until the lock is acquired.

Binary semaphores where a semaphore can be either 0 or 1, can also be called a mutex lock.

atomic values mean there is always something that happens before something else (not at the same time even if it looks like it).
# Actions Synchronization
## Slides

## Info
